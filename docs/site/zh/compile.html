<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>torch.compile - MiniCPM-o 4.5 文档</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
<style>
* { margin:0; padding:0; box-sizing:border-box; }
:root {
  --sidebar-w: 260px;
  --bg: #fff; --bg-side: #f8f9fa; --bg-code: #f6f8fa;
  --c1: #24292f; --c2: #57606a;
  --border: #d0d7de; --accent: #0969da; --nav-active: #ddf4ff;
}
body { font-family: -apple-system,BlinkMacSystemFont,"Segoe UI","Noto Sans",Helvetica,Arial,sans-serif; color:var(--c1); line-height:1.6; background:var(--bg); }
.sidebar-toggle { position:fixed; top:12px; left:12px; z-index:1001; background:var(--bg-side); border:1px solid var(--border); border-radius:6px; padding:6px 10px; font-size:18px; cursor:pointer; display:none; }
.sidebar { position:fixed; top:0; left:0; width:var(--sidebar-w); height:100vh; overflow-y:auto; background:var(--bg-side); border-right:1px solid var(--border); padding:20px 0; z-index:1000; transition:transform .3s; }
.sidebar-header { padding:0 20px 16px; border-bottom:1px solid var(--border); margin-bottom:8px; }
.sidebar-header h2 { font-size:16px; font-weight:600; }
.sidebar-subtitle { font-size:12px; color:var(--c2); }
.lang-switch { display:inline-block; margin-top:6px; font-size:12px; color:var(--accent); text-decoration:none; padding:2px 8px; border:1px solid var(--border); border-radius:4px; transition:background .15s; }
.lang-switch:hover { background:var(--nav-active); text-decoration:none; }

.nav-list { list-style:none; padding:0 8px; }
.nav-list > li > a { display:block; padding:7px 12px; color:var(--c2); text-decoration:none; font-size:14px; border-radius:6px; transition:background .15s,color .15s; }
.nav-list > li > a:hover { background:#e8e8e8; color:var(--c1); }
.nav-list > li > a.active { background:var(--nav-active); color:var(--accent); font-weight:600; }

.nav-group { margin-top:4px; }
.nav-group-header { display:flex; align-items:center; gap:6px; padding:8px 12px; color:var(--c1); font-size:12px; font-weight:700; text-transform:uppercase; letter-spacing:.04em; cursor:pointer; border-radius:6px; user-select:none; transition:background .15s; }
.nav-group-header:hover { background:#eaeef2; }
.nav-group-header::before { content:""; display:inline-block; width:0; height:0; border-left:5px solid var(--c2); border-top:3.5px solid transparent; border-bottom:3.5px solid transparent; transition:transform .2s; transform:rotate(90deg); flex-shrink:0; }
.nav-group.collapsed .nav-group-header::before { transform:rotate(0); }
.nav-group-children { list-style:none; margin:0 0 4px 19px; padding:3px 0 3px 13px; border-left:2px solid #e1e4e8; overflow:hidden; max-height:500px; transition:max-height .25s ease,opacity .2s ease,padding .2s ease; opacity:1; }
.nav-group.collapsed .nav-group-children { max-height:0; opacity:0; padding:0 0 0 13px; }
.nav-group-children li a { display:block; padding:4px 10px; font-size:13px; color:var(--c2); text-decoration:none; border-radius:4px; transition:background .15s,color .15s; }
.nav-group-children li a:hover { background:#e8e8e8; color:var(--c1); }
.nav-group-children li a.active { background:var(--nav-active); color:var(--accent); font-weight:600; }

.content { margin-left:var(--sidebar-w); max-width:900px; padding:40px 48px; }
article h1 { font-size:28px; font-weight:600; padding-bottom:10px; border-bottom:1px solid var(--border); margin-bottom:20px; }
article h2 { font-size:22px; font-weight:600; margin-top:32px; margin-bottom:12px; padding-bottom:6px; border-bottom:1px solid #eaecef; }
article h3 { font-size:18px; font-weight:600; margin-top:24px; margin-bottom:10px; }
article h4 { font-size:15px; font-weight:600; margin-top:20px; margin-bottom:8px; }
article p { margin-bottom:14px; }
article ul,article ol { margin-bottom:14px; padding-left:24px; }
article li { margin-bottom:4px; }
article a { color:var(--accent); text-decoration:none; }
article a:hover { text-decoration:underline; }
article code { background:var(--bg-code); padding:2px 6px; border-radius:4px; font-size:13px; font-family:"SFMono-Regular",Consolas,"Liberation Mono",Menlo,monospace; }
article pre { background:var(--bg-code); border:1px solid var(--border); border-radius:6px; padding:16px; overflow-x:auto; margin-bottom:16px; line-height:1.5; }
article pre code { background:none; padding:0; font-size:13px; }
article table { width:100%; border-collapse:collapse; margin-bottom:16px; font-size:14px; }
article th,article td { border:1px solid var(--border); padding:8px 12px; text-align:left; }
article th { background:var(--bg-code); font-weight:600; }
article tr:nth-child(even) { background:#f8f9fa; }
article hr { border:none; border-top:1px solid var(--border); margin:28px 0; }
article blockquote { border-left:4px solid var(--accent); padding:8px 16px; margin:0 0 16px; color:var(--c2); background:#f8f9fa; border-radius:0 6px 6px 0; }
article .mermaid { text-align:center; margin:20px 0; }
footer { margin-top:60px; padding-top:16px; border-top:1px solid var(--border); color:var(--c2); font-size:13px; }
@media(max-width:768px) {
  .sidebar { transform:translateX(-100%); }
  .sidebar.open { transform:translateX(0); box-shadow:2px 0 8px rgba(0,0,0,.15); }
  .sidebar-toggle { display:block; }
  .content { margin-left:0; padding:50px 20px 40px; }
  .content.shifted { margin-left:var(--sidebar-w); }
}
</style>
</head>
<body>
<button class="sidebar-toggle" onclick="toggleSidebar()" aria-label="Toggle sidebar">&#9776;</button>
<nav class="sidebar" id="sidebar">
  <div class="sidebar-header">
    <h2>MiniCPM-o 4.5</h2>
    <span class="sidebar-subtitle">项目文档</span>
    <a href="../en/compile.html" class="lang-switch">English</a>
  </div>
  <ul class="nav-list">
    <li><a href="index.html">项目概述</a></li>
    <li class="nav-group">
      <span class="nav-group-header">系统架构</span>
      <ul class="nav-group-children">
        <li><a href="architecture/index.html">架构概述</a></li>
        <li><a href="architecture/chat.html">Chat 模式</a></li>
        <li><a href="architecture/half-duplex.html">Half-Duplex 模式</a></li>
        <li><a href="architecture/duplex.html">Duplex 模式</a></li>
        <li><a href="architecture/internals.html">内部机制</a></li>
      </ul>
    </li>
    <li><a href="gateway.html">Gateway 模块</a></li>
    <li><a href="worker.html">Worker 模块</a></li>
    <li><a href="schema.html">Schema</a></li>
    <li><a href="model.html">模型模块</a></li>
    <li><a href="compile.html" class="active">torch.compile</a></li>
    <li class="nav-group">
      <span class="nav-group-header">前端模块</span>
      <ul class="nav-group-children">
        <li><a href="frontend/index.html">前端概述</a></li>
        <li><a href="frontend/pages.html">页面与路由</a></li>
        <li><a href="frontend/audio.html">音频处理</a></li>
        <li><a href="frontend/duplex-session.html">双工会话</a></li>
        <li><a href="frontend/components.html">UI 组件</a></li>
      </ul>
    </li>
    <li><a href="api.html">API 参考</a></li>
    <li><a href="deployment.html">配置与部署</a></li>
  </ul>
</nav>
<main class="content" id="content">
  <article><h1 id="torchcompile">torch.compile 加速</h1>
<h2 id="_1">概述</h2>
<p>在 A100、RTX 4090 等上一代 GPU 上，Omni Full-Duplex 模式的单 unit 计算耗时约 0.9s，接近 1 秒的实时阈值，会出现明显卡顿。</p>
<p><code>torch.compile</code> 通过 Triton 将核心子模块编译为优化后的 GPU kernel，可将计算耗时降至约 <strong>0.5s</strong>，满足实时要求，实现无卡顿的流畅交互。</p>
<h2 id="_2">启用方式</h2>
<p>在 <code>config.json</code> 中设置：</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;service&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;compile&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="_3">预编译（推荐）</h2>
<p><code>torch.compile</code> 的首次编译（冷启动）耗时约 15 分钟。为避免首次启动服务时的长时间等待，建议提前运行预编译脚本：</p>
<div class="codehilite"><pre><span></span><code><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span>.<span class="w"> </span>.venv/base/bin/python<span class="w"> </span>precompile.py
</code></pre></div>

<p>预编译生成的 Triton kernel 缓存保存在 <code>./torch_compile_cache</code> 目录下（由 <code>start_all.sh</code> 中的 <code>TORCHINDUCTOR_CACHE_DIR</code> 环境变量配置）。该缓存持久存储在磁盘上，后续所有启动都会自动复用，无需重复编译。</p>
<p>预编译完成后正常启动服务：</p>
<div class="codehilite"><pre><span></span><code><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3<span class="w"> </span>bash<span class="w"> </span>start_all.sh
</code></pre></div>

<p>Worker 从缓存加载约需 5 分钟（对比冷编译的 15 分钟）。</p>
<h2 id="_4">编译链路</h2>
<pre class="mermaid">
flowchart TB
    subgraph workerStart [Worker Startup]
        Load["load_model()"]
        UP["UnifiedProcessor.__init__()"]
        InitUnified["model.init_unified()"]
        Apply["model.apply_torch_compile()"]
        Warmup["model.warmup_compile()"]
        Ready["Worker IDLE"]
    end

    Load --> UP --> InitUnified --> Apply --> Warmup --> Ready

    subgraph applyDetail [apply_torch_compile]
        VPM["torch.compile(vpm)"]
        LLM["torch.compile(llm.model)"]
        RES["torch.compile(resampler)"]
        TTS["torch.compile(tts.model)"]
        TF32["set_float32_matmul_precision('high')"]
    end

    Apply --> VPM
    Apply --> LLM
    Apply --> RES
    Apply --> TTS
    Apply --> TF32

    subgraph warmupDetail [warmup_compile — Real Duplex Session]
        Extract["Extract MP4 audio + frames"]
        Prepare["duplex.prepare()"]
        Loop["Per-chunk loop:\nprefill → generate → finalize"]
        TTSfb["TTS fallback\n(if not triggered)"]
        Clean["Cleanup + empty_cache()"]
    end

    Warmup --> Extract --> Prepare --> Loop --> TTSfb --> Clean
</pre>

<h2 id="_5">编译目标</h2>
<h3 id="_6">被编译的子模块</h3>
<table>
<thead>
<tr>
<th>子模块</th>
<th>原始类</th>
<th>编译理由</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>vpm</code></td>
<td><code>SiglipVisionTransformer</code></td>
<td>视觉编码器，计算密集型 Transformer</td>
</tr>
<tr>
<td><code>llm.model</code></td>
<td><code>Qwen3Model</code></td>
<td>LLM 核心 backbone，推理主要耗时点</td>
</tr>
<tr>
<td><code>resampler</code></td>
<td><code>Resampler</code></td>
<td>视觉特征重采样，Perceiver 架构</td>
</tr>
<tr>
<td><code>tts.model</code></td>
<td><code>LlamaModel</code></td>
<td>TTS 核心 backbone，音频 token 生成</td>
</tr>
</tbody>
</table>
<p>只编译内层 backbone（如 <code>llm.model</code>），不编译外层 wrapper（如 <code>Qwen3ForCausalLM</code>），因为外层包含 Python 控制流（<code>generate()</code> 循环），编译收益低且容易导致 graph break。</p>
<h3 id="_7">不编译的部分</h3>
<table>
<thead>
<tr>
<th>子模块</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>apm</code>（Whisper 音频编码器）</td>
<td>streaming 特殊行为 + 动态 shape，compile 收益低</td>
</tr>
<tr>
<td><code>tts.audio_tokenizer</code>（Token2Wav/CosyVoice2）</td>
<td>外部库，非标准 <code>nn.Module</code></td>
</tr>
<tr>
<td><code>MiniCPMO</code> 外层</td>
<td>Python 控制流多（chat/streaming/duplex 分支），compile 收益低</td>
</tr>
<tr>
<td><code>lm_head</code></td>
<td>在外层 wrapper 中，generate 循环内调用</td>
</tr>
</tbody>
</table>
<h2 id="_8">编译参数</h2>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>mode</code></td>
<td><code>"default"</code></td>
<td>编译模式</td>
</tr>
<tr>
<td><code>dynamic</code></td>
<td><code>True</code></td>
<td>启用动态 shape 支持</td>
</tr>
</tbody>
</table>
<h3 id="_9">编译模式</h3>
<table>
<thead>
<tr>
<th>模式</th>
<th>编译耗时</th>
<th>运行速度</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>default</code></td>
<td>中等</td>
<td>较快</td>
<td>推荐，平衡编译时间和运行速度</td>
</tr>
<tr>
<td><code>reduce-overhead</code></td>
<td>中等</td>
<td>最快</td>
<td>使用 CUDA Graphs，仅适合静态 shape</td>
</tr>
<tr>
<td><code>max-autotune</code></td>
<td>很长</td>
<td>最快</td>
<td>最大优化，编译时间可能数分钟</td>
</tr>
</tbody>
</table>
<p>项目默认使用 <code>mode="default", dynamic=True</code>，因为推理场景中序列长度、图像尺寸等是动态变化的，<code>dynamic=True</code> 避免 shape 变化时重新编译。</p>
<h3 id="tf32">TF32 精度提升</h3>
<p><code>torch.set_float32_matmul_precision("high")</code> 启用 TF32 矩阵乘法，在 Ampere+ GPU 上可额外加速约 5-10%，精度损失可忽略。</p>
<h2 id="warmup_compile-duplex">warmup_compile() — 真实 Duplex 预热</h2>
<p><code>torch.compile</code> 只是包装，<strong>实际的 Triton 内核编译在首次 forward 时触发</strong>。<code>warmup_compile()</code> 使用真实 MP4 视频跑一遍完整的 Omni Full-Duplex 推理（prepare → prefill → generate → finalize），在真实推理链路中触发所有 compiled 子模块的 Triton 编译。</p>
<h3 id="_10">预热流程</h3>
<ol>
<li><strong>从 MP4 提取素材</strong> — ffmpeg 提取 16kHz 音频按 1s 切 chunk，每秒取 1 帧</li>
<li><strong>加载参考音频</strong> — TTS 声音克隆用</li>
<li><strong>启动 Duplex 会话</strong> — <code>duplex.prepare()</code> 初始化 StreamDecoder、预填 system prompt + ref audio</li>
<li><strong>逐 chunk 推理</strong> — 每个 chunk 执行 <code>streaming_prefill</code> → <code>streaming_generate</code> → <code>finalize_unit</code>，触发 vpm / resampler / llm / tts 的 Triton 编译</li>
<li><strong>TTS fallback</strong> — 若 duplex 全程 LISTEN 未触发 TTS，用合成数据单独预热 tts.model</li>
<li><strong>清理</strong> — 重置 duplex 状态、释放 token2wav 缓存、<code>torch.cuda.empty_cache()</code></li>
</ol>
<h3 id="_11">预热日志</h3>
<p>每个 unit 会打印详细的阶段耗时：</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">warmup</span><span class="o">]</span><span class="w"> </span><span class="n">unit</span><span class="o">=</span><span class="mi">0</span><span class="o">/</span><span class="mi">10</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="nl">prefill</span><span class="p">:</span><span class="w"> </span><span class="n">vis_proc</span><span class="o">=</span><span class="mi">120</span><span class="n">ms</span><span class="w"> </span><span class="n">vis_emb</span><span class="o">=</span><span class="mi">8500</span><span class="n">ms</span><span class="w"> </span><span class="n">vis_feed</span><span class="o">=</span><span class="mi">350</span><span class="n">ms</span>
<span class="w">  </span><span class="n">aud_proc</span><span class="o">=</span><span class="mi">45</span><span class="n">ms</span><span class="w"> </span><span class="n">aud_emb</span><span class="o">=</span><span class="mi">1200</span><span class="n">ms</span><span class="w"> </span><span class="n">aud_feed</span><span class="o">=</span><span class="mi">180</span><span class="n">ms</span><span class="w"> </span><span class="n">total</span><span class="o">=</span><span class="mi">10500</span><span class="n">ms</span><span class="w"> </span><span class="o">|</span>
<span class="w">  </span><span class="nl">generate</span><span class="p">:</span><span class="w"> </span><span class="n">llm</span><span class="o">=</span><span class="mi">2100</span><span class="n">ms</span><span class="w"> </span><span class="n">tts_prep</span><span class="o">=</span><span class="mi">0</span><span class="n">ms</span><span class="w"> </span><span class="n">tts</span><span class="o">=</span><span class="mi">0</span><span class="n">ms</span><span class="w"> </span><span class="n">token2wav</span><span class="o">=</span><span class="mi">0</span><span class="n">ms</span><span class="w"> </span><span class="n">total</span><span class="o">=</span><span class="mi">2200</span><span class="n">ms</span><span class="w"> </span><span class="o">|</span>
<span class="w">  </span><span class="n">decision</span><span class="o">=</span><span class="n">LISTEN</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">elapsed</span><span class="o">=</span><span class="mi">125</span><span class="n">s</span><span class="w"> </span><span class="n">remaining</span><span class="o">~</span><span class="mi">875</span><span class="n">s</span>
</code></pre></div>

<p>首个 chunk 因触发 Triton 编译耗时显著更长，后续 chunk 使用已编译的 kernel 会快很多。</p>
<h2 id="_12">缓存机制</h2>
<p>PyTorch Inductor 内置多层缓存，编译结果持久保存在 <code>TORCHINDUCTOR_CACHE_DIR</code> 目录下（项目默认 <code>./torch_compile_cache</code>）：</p>
<table>
<thead>
<tr>
<th>缓存层</th>
<th>作用</th>
<th>默认状态</th>
</tr>
</thead>
<tbody>
<tr>
<td>Inductor Kernel Cache</td>
<td>缓存生成的 Triton kernel .so 文件</td>
<td>默认开启</td>
</tr>
<tr>
<td>FX Graph Cache</td>
<td>缓存编译后的 FX 计算图</td>
<td>默认开启 (PyTorch 2.8+)</td>
</tr>
<tr>
<td>Autotune Cache</td>
<td>缓存 kernel 配置的 autotuning 结果</td>
<td>默认开启</td>
</tr>
</tbody>
</table>
<p>缓存在以下情况下失效，需要重新编译：
- PyTorch 版本升级
- CUDA / Triton 版本变更
- 模型代码结构变化（层数、结构等）
- GPU 架构变更（如从 A100 换到 H100）</p>
<h2 id="_13">调用链</h2>
<div class="codehilite"><pre><span></span><code><span class="n">config</span><span class="o">.</span><span class="n">json</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;service&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="s2">&quot;compile&quot;</span><span class="p">:</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="p">}</span>
<span class="w">    </span><span class="err">↓</span>
<span class="n">worker</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="w"> </span><span class="n">WORKER_CONFIG</span><span class="p">[</span><span class="s2">&quot;compile&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cfg</span><span class="o">.</span><span class="n">compile</span>
<span class="w">    </span><span class="err">↓</span>
<span class="n">MiniCPMOWorker</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">compile</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
<span class="w">    </span><span class="err">↓</span>
<span class="n">UnifiedProcessor</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">compile</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
<span class="w">    </span><span class="err">↓</span>
<span class="n">UnifiedProcessor</span><span class="o">.</span><span class="n">_load_model</span><span class="p">():</span>
<span class="w">    </span><span class="n">model</span><span class="o">.</span><span class="n">init_unified</span><span class="p">()</span>
<span class="w">    </span><span class="n">model</span><span class="o">.</span><span class="n">apply_torch_compile</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">dynamic</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
<span class="w">    </span><span class="n">model</span><span class="o">.</span><span class="n">warmup_compile</span><span class="p">(</span><span class="n">ref_audio_path</span><span class="o">=...</span><span class="p">)</span>
</code></pre></div>

<h2 id="duplexcapability">DuplexCapability 自动获益</h2>
<p><code>DuplexCapability</code> 通过<strong>引用</strong>访问模型的 <code>llm</code>, <code>vpm</code>, <code>tts</code> 等子模块，不持有独立副本。<code>apply_torch_compile()</code> 后，Duplex 推理自动使用已编译的版本，无需额外操作。</p>
<h2 id="_14">性能</h2>
<table>
<thead>
<tr>
<th>指标</th>
<th>不编译</th>
<th>编译后</th>
</tr>
</thead>
<tbody>
<tr>
<td>Omni Full-Duplex 单 unit 延迟 (A100)</td>
<td>~0.9s</td>
<td><strong>~0.5s</strong></td>
</tr>
<tr>
<td>首次启动额外耗时（冷编译）</td>
<td>—</td>
<td>~15 min</td>
</tr>
<tr>
<td>首次启动额外耗时（有缓存）</td>
<td>—</td>
<td>~5 min</td>
</tr>
<tr>
<td>运行时显存占用</td>
<td>~21.5 GB</td>
<td>~21.5 GB</td>
</tr>
</tbody>
</table>
<h2 id="_15">已知限制</h2>
<ul>
<li>某些极端输入 shape 可能触发 recompile</li>
<li>需要 PyTorch 2.x+，部分旧版 CUDA 驱动可能不兼容 Triton</li>
<li>编译后的代码难以单步调试</li>
</ul></article>
  <footer><p>MiniCPM-o 4.5 PyTorch Simple Demo &mdash; 由 build_docs.py 自动生成</p></footer>
</main>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/json.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script type="module">
import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
mermaid.initialize({startOnLoad:true,theme:'default',securityLevel:'loose'});
</script>
<script>
hljs.highlightAll();
function toggleSidebar(){document.getElementById('sidebar').classList.toggle('open');document.getElementById('content').classList.toggle('shifted');}
document.querySelectorAll('.nav-group-header').forEach(function(h){h.addEventListener('click',function(){this.parentElement.classList.toggle('collapsed');});});
document.addEventListener('click',function(e){var s=document.getElementById('sidebar'),t=document.querySelector('.sidebar-toggle');if(window.innerWidth<=768&&s.classList.contains('open')&&!s.contains(e.target)&&!t.contains(e.target)){s.classList.remove('open');document.getElementById('content').classList.remove('shifted');}});
</script>
</body>
</html>
