# MiniCPM-o 4.5 PyTorch 简易演示系统

[English Documentation](README.md)

本演示系统为 `MiniCPM-o 4.5` 模型训练团队官方提供的演示系统。本演示系统使用 PyTorch + CUDA 推理后端，结合简易的前后端设计，旨在以透明、简洁、无性能损失的方式，全面地演示 MiniCPM-o 4.5 的音视频全模态全双工能力。

本演示系统支持文本/图像/音频输入，实时语音输出。支持非流式、半双工和全双工交互模式。


| 模式 | 特点 
|------|------
| **Turn-based Chat (轮次对话)** | 低延迟流式交互，需要使用按钮或VAD（语音活动检测）触发回复，回复正确性好，基础能力强
| **Omnimodal Full-Duplex (全模态全双工)** | 全模态全双工实时交互，视觉语音输入、语音输出同时发生，模型完全自主决定说话时机，前沿能力强大
| **Audio Full-Duplex (语音全双工)** | 语音全双工实时交互，语音输入和语音输出同时发生，模型完全自主决定说话时机，前沿能力强大

目前支持的 3 种模式共享同一个模型实例，支持毫秒级热切换（< 0.1ms），近期将支持更多模式。

## 架构

```
Frontend (HTML/JS)
    |  HTTPS / WSS
Gateway (:8006, HTTPS)
    |  HTTP / WS (internal)
Worker Pool (:22400+)
    +-- Worker 0 (GPU 0)
    +-- Worker 1 (GPU 1)
    +-- ...
```

- **Frontend** — 模式选择首页、Turn-based Chat 轮次对话、Omni / Audio Duplex 全双工交互、Admin Dashboard 监控面板
- **Gateway** — 请求路由与分发、WebSocket 代理、请求排队与会话亲和
- **Worker** — 每 Worker 独占一张 GPU，支持 Turn-based Chat / Duplex 协议，Duplex 支持暂停/恢复（超时自动释放）


## 快速开始

### 检查系统要求
1. 确保你有一张显存大于 28GB 的 NVIDIA GPU。
2. 确保你的机器安装了 Linux 操作系统。

### 部署步骤
**1. 安装Python 3.10**

推荐使用 miniconda 安装 Python 3.10。

```bash
mkdir -p ./miniconda3_install_tmp

# 下载 miniconda3 安装脚本
wget https://repo.anaconda.com/miniconda/Miniconda3-py310_25.11.1-1-Linux-x86_64.sh -O ./miniconda3_install_tmp/miniconda.sh 

# 将 miniconda3 安装到项目目录下
bash ./miniconda3_install_tmp/miniconda.sh -b -u -p ./miniconda3 
```

安装完成后，会得到一个空的 base 环境，激活这个 base 环境，base 环境中默认为 Python 3.10。

```bash
source ./miniconda3/bin/activate
python --version # 应显示 3.10.x
```

**2. 安装 MiniCPM-o 4.5 所需的依赖**

使用项目目录下的 `install.sh` 安装依赖是最快的，它会在项目目录下的 .venv 中创建一个名为 `base` 的venv虚拟环境，并在其中安装所有的依赖。

```bash
source ./miniconda3/bin/activate
bash ./install.sh
```

如果网络良好，整个安装过程大约花费 5 分钟。如果你处在中国，可以考虑使用第三方 PyPi 镜像源，例如清华镜像源。

您也可以手动安装依赖，分 2 步：

```bash
# 首先准备好一个空的 python 3.10 环境
source ./miniconda3/bin/activate
python -m venv .venv/base
source .venv/base/bin/activate

# 安装 PyTorch。
pip install "torch==2.8.0" "torchaudio==2.8.0"

# 安装其余依赖。
pip install -r requirements.txt
```

**3. 创建配置文件**

将项目目录下的 `config.example.json` 复制为 `config.json`。

```bash
cp config.example.json config.json
```

模型路径（`model_path`），默认使用 `openbmb/MiniCPM-o-4_5`，如果你可以访问 huggingface，无需修改，将会自动从 huggingface 拉取模型。

<details>
<summary>点击展开关于模型路径的详细说明</summary>

(可选) 如果你习惯于下载模型权重到固定位置，或无法访问 huggingface，可以修改 model_path 为你的模型路径。
```bash
# 安装huggingface cli
pip install -U huggingface_hub

# 下载模型
huggingface-cli download openbmb/MiniCPM-o-4_5 --local-dir /path/to/your/MiniCPM-o-4_5

```

如果无法访问 huggingface，可以使用以下两种方式提前下载模型。

- 使用 hf-mirror 提前下载模型

```bash
pip install -U huggingface_hub

export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download openbmb/MiniCPM-o-4_5 --local-dir /path/to/your/MiniCPM-o-4_5
```

- 使用 modelscope 提前下载模型

```bash
pip install modelscope

modelscope download --model OpenBMB/MiniCPM-o-4_5 --local_dir /path/to/your/MiniCPM-o-4_5
```


</details>

<br/>

修改 `"gateway_port": 8006` 即可改变部署的端口，默认为 8006。


**4. 启动服务**
```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 bash start_all.sh
```

服务启动后访问 https://localhost:8006 即可。自签名证书会触发浏览器警告，点"高级"→"继续访问"。

<details>
<summary>点击展开关于启动选项的详细说明</summary>

以下是高级启动选项，目前供开发者参考。
```bash
CUDA_VISIBLE_DEVICES=0,1 bash start_all.sh          # 指定 GPU
bash start_all.sh --compile                          # torch.compile 加速（实验性，效果尚不稳定）
bash start_all.sh --http                             # 降级 HTTP（不推荐，麦克风/摄像头 API 需要 HTTPS）
```

**手动启动（分步）:**
```bash
# Worker（每张 GPU 一个）
CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. .venv/base/bin/python worker.py --worker-index 0 --gpu-id 0

# Gateway
PYTHONPATH=. .venv/base/bin/python gateway.py --port 10024 --workers localhost:22400
```
</details>

**5. 停止服务**：
```bash
pkill -f "gateway.py|worker.py"
```

<br/>
<br/>


## 已知问题和改进计划

- 轮次对话模式下，图片输入暂时不可用，仅支持音频和文本输入，近期会拆分出图片问答模式。
- 半双工的语音通话（无需按钮触发回复）正在开发中，近期合入。
- 语音全双工模式下，回声消除目前存在问题，影响到打断成功率，推荐使用耳机进行交互，近期将修复。
- 语音模式下，由于模型的训练策略，中文和英文通话下，需要使用对应语言的系统提示词。

<br/>

## 项目结构

**项目代码结构**
```
minicpmo45_service/
├── config.json               # 服务配置（从 config.example.json 复制，gitignored）
├── config.example.json       # 配置示例（完整字段 + 默认值）
├── config.py                 # 配置加载逻辑（Pydantic 定义 + JSON 加载）
├── requirements.txt          # Python 依赖
├── start_all.sh              # 一键启动脚本
│
├── gateway.py                # Gateway（路由、排队、WS 代理）
├── worker.py                 # Worker（推理服务）
├── gateway_modules/          # Gateway 业务模块
│
├── core/                     # 核心封装
│   ├── schemas/              # Pydantic Schema（请求/响应）
│   └── processors/           # 推理处理器（UnifiedProcessor）
│
├── MiniCPMO45/               # 模型核心推理代码
├── static/                   # 前端页面
├── resources/                # 资源文件（参考音频等）
├── tests/                    # 测试
└── tmp/                      # 运行时日志和 PID 文件
```

**前端路由设定**

| 页面 | URL |
|------|-----|
| 非流式 | https://localhost:8006 |
| 全模态全双工 | https://localhost:8006/omni |
| 语音全双工 | https://localhost:8006/audio_duplex |
| 仪表盘 | https://localhost:8006/admin |
| API 文档 | https://localhost:8006/docs |

<br/>
<br/>

## 配置说明

### config.json — 统一配置文件

所有配置集中在 `config.json`（从 `config.example.json` 复制）。
`config.json` 已 gitignore，不会被提交。

**配置优先级**：CLI 参数 > config.json > Pydantic 默认值

| 分组 | 字段 | 默认值 | 说明 |
|------|------|--------|------|
| **model** | `model_path` | _(必填)_ | HuggingFace 格式模型目录 |
| model | `pt_path` | null | 额外 .pt 权重覆盖 |
| model | `attn_implementation` | `"auto"` | Attention 实现：`"auto"`/`"flash_attention_2"`/`"sdpa"`/`"eager"` |
| **audio** | `ref_audio_path` | `assets/ref_audio/ref_minicpm_signature.wav` | 默认 TTS 参考音频 |
| audio | `playback_delay_ms` | 200 | 前端音频播放延迟（ms），越大越平滑但延迟越高 |
| audio | `chat_vocoder` | `"token2wav"` | Chat 模式 vocoder：`"token2wav"`（默认）或 `"cosyvoice2"` |
| **service** | `gateway_port` | 8006 | Gateway 端口 |
| service | `worker_base_port` | 22400 | Worker 起始端口 |
| service | `max_queue_size` | 100 | 最大排队请求数 |
| service | `request_timeout` | 300.0 | 请求超时（秒） |
| service | `compile` | false | torch.compile 加速 |
| service | `data_dir` | "data" | 数据目录 |
| **duplex** | `pause_timeout` | 60.0 | Duplex 暂停超时（秒） |

**最小配置**（只需模型路径）：
```json
{"model": {"model_path": "/path/to/model"}}
```

## CLI 参数覆盖

```bash
# Worker
python worker.py --model-path /alt/model --pt-path /alt/weights.pt --ref-audio-path /alt/ref.wav --compile

# Gateway
python gateway.py --port 10025 --workers localhost:22400,localhost:22401 --http
```


## 资源消耗

| 资源 | Token2Wav（默认）
|------|-------------------
| 显存（每 Worker，初始化完成后） | ~21.5 GB
| 模型加载时间 | ~16s
| 模式切换延迟 | < 0.1ms

> compile 模式首次推理额外 ~60s 编译耗时。

## 测试

```bash

# Schema 单元测试（无需 GPU）
PYTHONPATH=. .venv/base/bin/python -m pytest tests/test_schemas.py -v

# Processor 测试（需要 GPU）
CUDA_VISIBLE_DEVICES=0 PYTHONPATH=. .venv/base/bin/python -m pytest tests/test_chat.py tests/test_streaming.py tests/test_duplex.py -v -s

# API 集成测试（需要先启动服务）
PYTHONPATH=. .venv/base/bin/python -m pytest tests/test_api.py -v -s
```
